{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import warnings as warnings\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import plotnine as gg\n",
    "\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Imputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "gg.theme_set(gg.theme_bw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/engineered_factset_campaign_data.csv', parse_dates=['campaign_announcement_date'])\n",
    "\n",
    "df_objective_mapping = pd.read_csv('../mapping/campaign_mapping.csv')\n",
    "df_value_mapping = pd.read_csv('../mapping/value_demand_mapping.csv')\n",
    "df_governance_mapping = pd.read_csv('../mapping/governance_demand_mapping.csv')\n",
    "df_result_mapping = pd.read_csv('../mapping/proxy_result_mapping.csv')\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .assign(has_value_demand=lambda df: 1 * df.value_demand.notnull())\n",
    "    .assign(has_governance_demand=lambda df: 1 * df.governance_demand.notnull())\n",
    "    .assign(glass_lewis_support_indicator=lambda df: 1 * (df.glass_lewis_support == 'Management').fillna(0))\n",
    "    .assign(iss_support_indicator=lambda df: 1 * (df.iss_support == 'Management').fillna(0))\n",
    "    .assign(independent_support_indicator=lambda df: 1 * (df.glass_lewis_support_indicator | df.iss_support_indicator))\n",
    "    .assign(poison_pill_indicator=lambda df: 1 * (df.poison_pill_in_force_prior_to_announcement == \"Yes\").fillna(0))\n",
    "    .assign(poison_pill_adopted_indicator=lambda df: 1 * (df.poison_pill_adopted_in_response_to_campaign == \"Yes\").fillna(0))\n",
    "    .assign(cumulative_6m_residual_return=lambda df: df.cumulative_6m_residual_return.clip(-0.30, 0.30))\n",
    "    .assign(ownership_pecent_on_announcement=lambda df: df.ownership_pecent_on_announcement.fillna(0).clip(0, 0.10))\n",
    "    .assign(campaign_return=lambda df: df.cumulative_6m_residual_return)\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .assign(board_seats_percentage_sought=lambda df: np.where(\n",
    "        df.total_number_of_board_seats > 0,\n",
    "        (df.number_of_board_seats_sought / df.total_number_of_board_seats).fillna(0),\n",
    "        np.nan\n",
    "    ))\n",
    "    .assign(board_seat_percentage_gained=lambda df: np.where(\n",
    "        df.number_of_board_seats_sought > 0,\n",
    "        df.number_of_board_seats_gained.fillna(0) / df.number_of_board_seats_sought,\n",
    "        np.nan\n",
    "    ))\n",
    "    .assign(board_seat_result_group = lambda df: np.select(\n",
    "        [\n",
    "            df.board_seat_percentage_gained >= 1,\n",
    "            df.board_seat_percentage_gained > 0,\n",
    "            df.board_seat_percentage_gained <= 0,\n",
    "            df.board_seat_percentage_gained.isnull(),\n",
    "        ],\n",
    "        [\n",
    "            'Dissident',\n",
    "            'Dissident',\n",
    "            'Management',\n",
    "            None\n",
    "        ]\n",
    "    ))\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .pipe(pd.merge, df_objective_mapping, how='left', on='campaign_objective_primary')\n",
    "    .pipe(pd.merge, df_value_mapping, how='left', on='value_demand')\n",
    "    .pipe(pd.merge, df_governance_mapping, how='left', on='governance_demand')\n",
    "    .pipe(pd.merge, df_result_mapping, how='left', on='proxy_campaign_winner_or_result')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['campaign_objective_group'],\n",
    "    prefix='campaign_objective',\n",
    "    prefix_sep='=',\n",
    "    drop_first=True,\n",
    "    dummy_na=False\n",
    ")\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['value_demand_group'],\n",
    "    prefix='value_demand',\n",
    "    prefix_sep='=',\n",
    "    drop_first=True,\n",
    "    dummy_na=False\n",
    ")\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['governance_demand_group'],\n",
    "    prefix='governance_demand',\n",
    "    prefix_sep='=',\n",
    "    drop_first=True,\n",
    "    dummy_na=False\n",
    ")\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['proxy_result_group'],\n",
    "    prefix='proxy_result',\n",
    "    prefix_sep='=',\n",
    "    drop_first=True,\n",
    "    dummy_na=False\n",
    ")\n",
    "\n",
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['board_seat_result_group'],\n",
    "    prefix='board_result',\n",
    "    prefix_sep='=',\n",
    "    drop_first=True,\n",
    "    dummy_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.board_seat_percentage_gained.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.proxy_result_code.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pre_12m_dividends.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='pre_12m_dividends', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = (\n",
    "    [\n",
    "        'ownership_pecent_on_announcement',\n",
    "        'past_return_successes',\n",
    "        'independent_support_indicator'\n",
    "    ] +\n",
    "    [c for c in df.columns.tolist() if 'used_' in c] +\n",
    "    [c for c in df.columns.tolist() if 'campaign_objective=' in c] +\n",
    "    [c for c in df.columns.tolist() if 'value_demand=' in c] +\n",
    "    [c for c in df.columns.tolist() if 'governance_demand=' in c] +\n",
    "    [c for c in df.columns.tolist() if 'proxy_result=' in c] +\n",
    "    [c for c in df.columns.tolist() if 'board_result=' in c] +\n",
    "    [\n",
    "        # target\n",
    "        'total_number_of_board_seats',\n",
    "        'board_seats_percentage_sought',\n",
    "        'poison_pill_indicator',\n",
    "        'poison_pill_adopted_indicator',\n",
    "        'pre_12m_earnings_yield',\n",
    "        'beta'\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_column = [\n",
    "    'campaign_return'\n",
    "]\n",
    "\n",
    "df_lm = df.dropna(subset=y_column)\n",
    "\n",
    "df_train = df_lm[df_lm.campaign_announcement_date <= '2016-12-31']\n",
    "df_test = df_lm[df_lm.campaign_announcement_date >= '2017-01-01']\n",
    "\n",
    "df_full = df_lm[x_columns + y_column]\n",
    "X_train, y_train = df_train[x_columns], df_train[y_column]\n",
    "X_test, y_test = df_test[x_columns], df_test[y_column]\n",
    "\n",
    "n_samples, n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Full data set has {len(df)} observations\")\n",
    "print(f\"Regression data set has {len(df_full)} observations\")\n",
    "print(\"Train Feature shape: {}\".format(X_train.shape))\n",
    "print(\"Train Target shape: {}\".format(y_train.shape))\n",
    "print(\"Test Feature shape: {}\".format(X_test.shape))\n",
    "print(\"Test Target shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_full == np.inf).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clean, X_train_clean = y_train[X_train.notnull().all(axis='columns')], X_train[X_train.notnull().all(axis='columns')]\n",
    "sm.OLS(y_train_clean, sm.add_constant(X_train_clean)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [c for c in df_full.dtypes[df_full.dtypes != float].index.tolist() if c not in y_column]\n",
    "numeric_features = [c for c in df_full.dtypes[df_full.dtypes == float].index.tolist() if c not in y_column]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features),\n",
    "    ('categorical', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "fitted_model = model.fit(X_train, y_train)\n",
    "y_predicted = fitted_model.predict(X_test)\n",
    "df_predicted = df_test.assign(predicted_campaign_return=y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = fitted_model.score(X_train, y_train)\n",
    "r2_test = fitted_model.score(X_test, y_test)\n",
    "print(f\"Training set score is {r2_train}\")\n",
    "print(f\"Test set score is {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gg.ggplot(df_predicted) +\n",
    "    gg.geom_point(gg.aes(x='predicted_campaign_return', y='campaign_return')) +\n",
    "    gg.theme(axis_text=gg.element_text(size=10, rotation=90), figure_size=(12, 6)) +\n",
    "    gg.labs(\n",
    "        title = \"Predicted vs. Realized\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coefficients = (\n",
    "    pd.DataFrame({\n",
    "    'variable': X_train.columns.tolist(),\n",
    "    'coefficient': model.named_steps['model'].coef_.flatten().tolist(),\n",
    "    })\n",
    "    .sort_values('coefficient')\n",
    "    .assign(variable=lambda df: pd.Categorical(df.variable, categories=df.variable.tolist()))\n",
    ")\n",
    "\n",
    "(\n",
    "    gg.ggplot(df_coefficients) +\n",
    "    gg.geom_bar(gg.aes(x='variable', y='coefficient'), stat='identity', position='dodge') +\n",
    "    gg.theme(axis_text=gg.element_text(size=8, rotation=0), figure_size=(6, 12)) +\n",
    "    gg.coord_flip() +\n",
    "    gg.labs(\n",
    "        title = \"Coefficients\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proper scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [c for c in df_full.dtypes[df_full.dtypes != float].index.tolist() if c not in y_column]\n",
    "numeric_features = [c for c in df_full.dtypes[df_full.dtypes == float].index.tolist() if c not in y_column]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features),\n",
    "    ('categorical', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "dict_model_pipelines = {\n",
    "    \"Model 1 - Linear Regression\": [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", LinearRegression())\n",
    "    ],\n",
    "    \"Model 2 - Ridge Regression\": [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", Ridge())\n",
    "    ],\n",
    "    \"Model 3 - Lasso Regression\": [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", Lasso())\n",
    "    ]\n",
    "}\n",
    "\n",
    "dict_model_parameter_grids = {\n",
    "    \"Model 1 - Linear Regression\": {},\n",
    "    \"Model 2 - Ridge Regression\": {\n",
    "        'model__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    },\n",
    "    \"Model 3 - Lasso Regression\": {\n",
    "        'model__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    }\n",
    "}\n",
    "\n",
    "dict_grid_search_params = {\n",
    "    \"scoring\": 'r2',\n",
    "    \"cv\": 5,\n",
    "    \"refit\": True,\n",
    "    \"iid\": True\n",
    "}\n",
    "\n",
    "dict_cv_params = {\n",
    "    \"scoring\": 'r2',\n",
    "    \"cv\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models_baseline = {}\n",
    "dict_grids = {}\n",
    "dict_models_optimized = {}\n",
    "dict_scores = {}\n",
    "dict_train_predicted = {}\n",
    "dict_test_predicted = {}\n",
    "\n",
    "for model_name, model_pipeline_parameters in dict_model_pipelines.items():\n",
    "    \n",
    "    print('running model {}...'.format(model_name))\n",
    "    print('=' * 100)\n",
    "    print('')\n",
    "    \n",
    "    # model parameters\n",
    "    \n",
    "    pipeline_baseline_parameters = dict_model_pipelines[model_name].copy()\n",
    "    grid_search_parameters = dict_model_parameter_grids[model_name].copy()\n",
    "    pipeline_optimized_parameters = dict_model_pipelines[model_name].copy()  # will be updated\n",
    "    \n",
    "    # baseline fit on full training data\n",
    "    \n",
    "    model_pipeline_basline = Pipeline(pipeline_baseline_parameters)\n",
    "    model_pipeline_basline = model_pipeline_basline.fit(X_train, y_train)\n",
    "\n",
    "    params_baseline = model_pipeline_basline.get_params(deep=True)\n",
    "    \n",
    "    score_baseline_train = model_pipeline_basline.score(X_train, y_train)\n",
    "    score_baseline_validation = np.mean(cross_val_score(model_pipeline_basline, X_train, y_train, **dict_cv_params))\n",
    "    score_baseline_test = model_pipeline_basline.score(X_test, y_test)\n",
    "    \n",
    "    print('simple fit on training sample')\n",
    "    print('-' * 80)\n",
    "    print('')\n",
    "    print('full-sample baseline parameters: {}'.format(params_baseline['model']))\n",
    "    print('full-sample baseline train score: {}'.format(score_baseline_train))\n",
    "    print('full-sample baseline validation score: {}'.format(score_baseline_validation))\n",
    "    print('full-sample baseline test score: {}'.format(score_baseline_test))\n",
    "    print('')\n",
    "    \n",
    "    # grid fit on cv-samples of training data\n",
    "\n",
    "    model_grid_pipeline = Pipeline(pipeline_baseline_parameters)\n",
    "    model_grid = GridSearchCV(model_grid_pipeline, param_grid=grid_search_parameters, **dict_grid_search_params)\n",
    "    model_grid = model_grid.fit(X_train, y_train)\n",
    "    \n",
    "    params_grid_best = model_grid.best_params_.copy()\n",
    "    \n",
    "    score_grid_train = model_grid.score(X_train, y_train)\n",
    "    score_grid_validation = np.mean(cross_val_score(model_grid, X_train, y_train, **dict_cv_params))\n",
    "    score_grid_test = model_grid.score(X_test, y_test)\n",
    "\n",
    "    print('cv fit on training sample')\n",
    "    print('-' * 80)\n",
    "    print('')\n",
    "    print('grid-search-cv best parameters: {}'.format(params_grid_best))\n",
    "    print('grid-search-cv best train score: {}'.format(score_grid_train))\n",
    "    print('grid-search-cv best validation score: {}'.format(score_grid_validation))\n",
    "    print('grid-search-cv best test score: {}'.format(score_grid_test))\n",
    "    print('')\n",
    "    \n",
    "    # optimized fit on full training data\n",
    "    \n",
    "    model_pipeline_optimized = Pipeline(pipeline_optimized_parameters)\n",
    "    model_pipeline_optimized.set_params(**params_grid_best)\n",
    "    model_pipeline_optimized = model_pipeline_optimized.fit(X_train, y_train)\n",
    "\n",
    "    params_optimized = model_pipeline_optimized.get_params(deep=True)\n",
    "    \n",
    "    score_optimized_train = model_pipeline_optimized.score(X_train, y_train)\n",
    "    score_optimized_validation = np.mean(cross_val_score(model_pipeline_optimized, X_train, y_train, **dict_cv_params))\n",
    "    score_optimized_test = model_pipeline_optimized.score(X_test, y_test)\n",
    "    \n",
    "    print('optimized fit on training sample')\n",
    "    print('-' * 80)\n",
    "    print('')\n",
    "    print('full-sample optimized parameters: {}'.format(params_optimized['model']))\n",
    "    print('full-sample optimized train score: {}'.format(score_optimized_train))\n",
    "    print('full-sample optimized validation score: {}'.format(score_optimized_validation))\n",
    "    print('full-sample optimized test score: {}'.format(score_optimized_test))\n",
    "    print('')\n",
    "    \n",
    "    # final prediction\n",
    "    \n",
    "    y_train_predicted = model_pipeline_optimized.predict(X_train)\n",
    "    y_test_predicted = model_pipeline_optimized.predict(X_test)\n",
    "    \n",
    "    df_train_predicted = df_train.assign(predicted_campaign_return=y_train_predicted).assign(model_name=model_name)\n",
    "    df_test_predicted = df_test.assign(predicted_campaign_return=y_test_predicted).assign(model_name=model_name)\n",
    "\n",
    "    # store\n",
    "    dict_models_baseline[model_name] = model_pipeline_basline\n",
    "    dict_grids[model_name] = model_grid.cv_results_\n",
    "    dict_models_optimized[model_name] = model_pipeline_optimized\n",
    "    dict_scores[model_name] = pd.DataFrame(\n",
    "        columns=['IsTuned', 'Sample', 'Score'],\n",
    "        data={\n",
    "            'IsTuned': [\n",
    "                False, False, False,\n",
    "                True, True, True\n",
    "            ],\n",
    "            'Sample': [\n",
    "                'Train', 'Validation', 'Test',\n",
    "                'Train', 'Validation', 'Test'\n",
    "            ],\n",
    "            'Score': [\n",
    "                score_baseline_train, score_baseline_validation, score_baseline_test,\n",
    "                score_optimized_train, score_optimized_validation, score_optimized_test\n",
    "            ]\n",
    "        },\n",
    "    ).set_index(['IsTuned', 'Sample'])\n",
    "    dict_train_predicted[model_name] = df_train_predicted\n",
    "    dict_test_predicted[model_name] = df_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.concat(dict_scores, names=['Model', 'IsTuned', 'Sample']).unstack('IsTuned')\n",
    "df_train_predictions = pd.concat(dict_train_predicted.values())\n",
    "df_test_predictions = pd.concat(dict_test_predicted.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    gg.ggplot(df_train_predictions) +\n",
    "    gg.geom_point(gg.aes(x='predicted_campaign_return', y='campaign_return', color='model_name')) +\n",
    "    gg.theme(axis_text=gg.element_text(size=10, rotation=90), figure_size=(12, 6)) +\n",
    "    gg.labs(\n",
    "        title = \"Predicted vs. Realized\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coefficients = (\n",
    "    pd.concat({\n",
    "        model_name: pd.DataFrame({\n",
    "            'variable': X_train.columns.tolist(),\n",
    "            'coefficient': model.named_steps['model'].coef_.flatten().tolist()\n",
    "        })\n",
    "        for model_name, model in dict_models_optimized.items()\n",
    "    }, names=['model_name', 'variable_id'])\n",
    "    .reset_index()\n",
    "    .sort_values(['model_name', 'coefficient'])\n",
    "    .assign(variable=lambda df: pd.Categorical(df.variable, categories=df.variable.unique().tolist()))\n",
    ")\n",
    "\n",
    "(\n",
    "    gg.ggplot(df_coefficients) +\n",
    "    gg.geom_bar(gg.aes(x='variable', y='coefficient', fill='model_name'), stat='identity', position='dodge') +\n",
    "    gg.theme(axis_text=gg.element_text(size=8, rotation=0), figure_size=(6, 12)) +\n",
    "    gg.coord_flip() +\n",
    "    gg.labs(\n",
    "        title = \"Coefficients\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_data = (\n",
    "    df\n",
    "    .assign(campaign_objective_group=lambda df: df.campaign_objective_group.fillna('(Missing)'))\n",
    "    .assign(value_demand_group=lambda df: df.value_demand_group.fillna('(Missing)'))\n",
    "    .assign(governance_demand_group=lambda df: df.governance_demand_group.fillna('(Missing)'))\n",
    "    .assign(ownership_exceeds_5_indicator=lambda df: 1 * (df.ownership_pecent_on_announcement > 0.05).fillna(0))\n",
    "    .assign(campaign_outcome_is_management=lambda df: 1 * ((df.proxy_result_group == \"Management\") | (df.board_seat_result_group == \"Management\")))\n",
    "    .assign(campaign_return=lambda df: df.cumulative_6m_residual_return)\n",
    "    .assign(campaign_return_is_positive=lambda df: 1 * (df.campaign_return > 0))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = (\n",
    "    [\n",
    "        'ownership_exceeds_5_indicator',\n",
    "        'poison_pill_indicator',\n",
    "        'poison_pill_adopted_indicator',\n",
    "        'independent_support_indicator'\n",
    "    ] \n",
    ")\n",
    "\n",
    "y1_column = ['campaign_outcome_is_management']\n",
    "y2_column = ['campaign_return_is_positive']\n",
    "\n",
    "all_columns = x_columns + y1_column + y2_column\n",
    "\n",
    "df_train = df_pgm_data[df_pgm_data.campaign_announcement_date <= '2016-12-31'].dropna(subset=y2_column).loc[:, all_columns]\n",
    "df_test = df_pgm_data[df_pgm_data.campaign_announcement_date >= '2017-01-01'].dropna(subset=y2_column).loc[:, x_columns + y1_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_data.groupby(['campaign_outcome', 'campaign_return_bin']).campaign_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pgm_data.groupby(['campaign_outcome', 'campaign_return_bin']).campaign_return.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianModel(\n",
    "    [(x_column, y1_column[0]) for x_column in x_columns] +\n",
    "    [(y1_column[0], y2_column[0])]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    df_train,\n",
    "    estimator=BayesianEstimator,\n",
    "    complete_samples_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cpd in model.get_cpds():\n",
    "    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n",
    "    print(cpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_independencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sample = df_test.sample(5)\n",
    "df_test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(df_test_sample)\n",
    "df_test_predicted = df_test_sample.assign(campaign_return_predicted=y_predicted)\n",
    "df_test_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_predicted_probabilities = model.predict_probability(df_test_sample)\n",
    "df_test_predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pylab as plt\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw(model, with_labels=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
